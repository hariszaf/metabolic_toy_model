{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hariszaf/metabolic_toy_model/blob/main/DNNGIORtraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8LbxwOozGpm"
      },
      "source": [
        "# **Use DNNGIOR On Your Own Training Set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfYBcGfpaMjl"
      },
      "source": [
        "## Google Collab part"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install dependencies\n",
        "!pip install tensorflow==2.15.1\n",
        "!pip install cobra\n",
        "!pip install dnngior --no-deps\n",
        "!pip install gurobipy\n",
        "!git clone https://github.com/hariszaf/metabolic_toy_model.git"
      ],
      "metadata": {
        "cellView": "form",
        "id": "unPUjtj2Bc8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title License\n",
        "import os\n",
        "def create_gurobi_license():\n",
        "    license_content = (\n",
        "        \"# Gurobi WLS license file\\n\"\n",
        "        \"# Your credentials are private and should not be shared or copied to public repositories.\\n\"\n",
        "        \"# Visit https://license.gurobi.com/manager/doc/overview for more information.\\n\"\n",
        "        \"WLSACCESSID=\\n\"\n",
        "        \"WLSSECRET=\\n\"\n",
        "        \"LICENSEID=\"\n",
        "    )\n",
        "    with open(\"/content/licenses/gurobi.lic\", \"w\") as f:\n",
        "        f.write(license_content)\n",
        "    print(\"License file created at /content/licenses/gurobi.lic\")\n",
        "\n",
        "# Create directory for the license\n",
        "os.makedirs(\"/content/licenses\", exist_ok=True)\n",
        "\n",
        "# Generate the license file\n",
        "create_gurobi_license()\n",
        "\n",
        "#add to path\n",
        "os.environ['GRB_LICENSE_FILE'] = '/content/licenses/gurobi.lic'\n",
        "\n",
        "!pip install gurobipy"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_hIKdXCLC0E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd metabolic_toy_model/Antony2025/"
      ],
      "metadata": {
        "id": "EW9tsKKEfWgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "FTZSVRHsdIqG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxlLifb0RO-J"
      },
      "source": [
        "For this workshop we are going to use the NN_Trainer, NN_Predictor and Gapfill classes of DNNGIOR as well as the read_sbml_model from cobra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNT_SUqwRO-J"
      },
      "outputs": [],
      "source": [
        "from dnngior import NN_Trainer\n",
        "from dnngior import NN_Predictor\n",
        "from dnngior import Gapfill\n",
        "\n",
        "#general dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from cobra.io import read_sbml_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXPz08uLRO-K"
      },
      "source": [
        "## Generating reaction presence dataframe\n",
        "\n",
        "By default DNNGIOR uses a neural network trained on a best-per-species dataset, that has learned the coocurences of reactions encoded by the genomes. If we want to train our own network we need to prepare our own training data we need to take a set genomes and determine the reactions present. The easiest way for this is to take the draft models of our genomes and generate a binary array encoding which models contain which reactions.\n",
        "\n",
        "![training](https://github.com/hariszaf/metabolic_toy_model/blob/main/Antony2025/images/training.png?raw=1)\n",
        "\n",
        "We need a list of all possible reactions found in our training data, which will serve as the rows in our dataset. We will keep track of the reactions for every model in a dictionary for the first loop where we are determining the reactome of our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1y8QYx2l0Tw"
      },
      "outputs": [],
      "source": [
        "#path to training models\n",
        "model_path =  './files/models/one_per_phylum_models/'\n",
        "\n",
        "#list of model-ids of draft-models\n",
        "paths  = os.listdir(model_path)[:3]\n",
        "model_ids = []\n",
        "for filename in paths:\n",
        "    model_ids.append(filename[:-5])\n",
        "\n",
        "\n",
        "model_reaction_dic = {}\n",
        "rxn = []\n",
        "for file_path, model_id in zip(paths,model_ids):\n",
        "    print(model_id)\n",
        "    model = read_sbml_model(os.path.join(model_path, file_path))\n",
        "    rs = set(model.reactions.list_attr('id'))\n",
        "    model_reaction_dic[model_id]=rs\n",
        "\n",
        "    #generate a list of all possible reactions\n",
        "    for i in list(rs):\n",
        "         if i not in rxn and not i.startswith('EX_'):\n",
        "             rxn.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46r2lZAPmAYe"
      },
      "source": [
        "We can then determine for every draft training models which of these reactions are present and create a binary list of reactions presences. We will end up with a binary array with as rows the different reactions and as columns all models in the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCkM7IyjmDmL"
      },
      "outputs": [],
      "source": [
        "#output path training data\n",
        "output_path = 'one_per_phylum_training_data.csv'\n",
        "\n",
        "reaction_df=pd.DataFrame(index=rxn, columns=model_ids)\n",
        "\n",
        "for key, value in model_reaction_dic.items():\n",
        "    a = []\n",
        "    for i in rxn:\n",
        "        if i in value:\n",
        "            a.append(1)\n",
        "        else:\n",
        "            a.append(0)\n",
        "    reaction_df[key]=a\n",
        "\n",
        "#saving to pandas file\n",
        "reaction_df.to_csv(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIvsS5iSmoWX"
      },
      "source": [
        "We can have a look at the distribution of reaction set sizes to see if the extraction was succesful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-jk3N1fRO-M"
      },
      "outputs": [],
      "source": [
        "reaction_df = pd.read_csv('./files/one_per_phylum_training_data.csv', index_col=0)\n",
        "reaction_df.sum().hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FetNGRve3gI"
      },
      "source": [
        "### Challenge:\n",
        "\n",
        "Create a training dataset based on 5 Shewanella models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY4E5EZLHDJU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Solution\n",
        "\n",
        "#path to training models\n",
        "model_path =  './files/models/shewanella_models'\n",
        "\n",
        "#list of model-ids of draft-models\n",
        "paths  = os.listdir(model_path)\n",
        "model_ids = []\n",
        "for filename in paths:\n",
        "    model_ids.append(filename[:-5])\n",
        "\n",
        "\n",
        "shewanella_model_reaction_dic = {}\n",
        "rxn = []\n",
        "for file_path, model_id in zip(paths,model_ids):\n",
        "    print(model_id)\n",
        "    model = read_sbml_model(os.path.join(model_path, file_path))\n",
        "    rs = set(model.reactions.list_attr('id'))\n",
        "    shewanella_model_reaction_dic[model_id]=rs\n",
        "\n",
        "    #generate a list of all possible reactions\n",
        "    for i in list(rs):\n",
        "         if i not in rxn and not i.startswith('EX_'):\n",
        "             rxn.append(i)\n",
        "\n",
        "#output path training data\n",
        "output_path = 'shewanella_training_data.csv'\n",
        "\n",
        "shewanella_reaction_df=pd.DataFrame(index=rxn, columns=model_ids)\n",
        "\n",
        "for key, value in shewanella_model_reaction_dic.items():\n",
        "    a = []\n",
        "    for i in rxn:\n",
        "        if i in value:\n",
        "            a.append(1)\n",
        "        else:\n",
        "            a.append(0)\n",
        "    shewanella_reaction_df[key]=a\n",
        "\n",
        "#saving to pandas file\n",
        "shewanella_reaction_df.to_csv(output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQde0hb4RO-O"
      },
      "source": [
        "## Training the Neural Network\n",
        "\n",
        "Now that we have prepared the training data, we can start training the network. During training the network will generate the feature by randomly deleting reactions from the input data and then try to predict the missing reactions.\n",
        "\n",
        "We do this using the train function of the `NN_Trainer` class. This function will return a object of the predictor class (`NN_predictor`) containing the network, the reaction keys and modeltype. If a `output_path` is provided, it will also save it as a npz file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzp48PRVRO-O"
      },
      "outputs": [],
      "source": [
        "#Load in a our training sample\n",
        "training_data_path = './files/one_per_phylum_training_data.csv'\n",
        "training_data = pd.read_csv(training_data_path, index_col=0)\n",
        "\n",
        "#set save path\n",
        "save_path = 'NN_phylum.npz'\n",
        "\n",
        "#Train the network\n",
        "NN_example = NN_Trainer.train(data=training_data, modeltype='ModelSEED',output_path=save_path)\n",
        "\n",
        "print(\"The weights of the first layer of the network: \\n{}\".format(NN_example.network[0][0]))\n",
        "print(\"The rxn_keys: \\n{}\".format(NN_example.rxn_keys.values))\n",
        "print(\"The Modeltype: {}\".format(NN_example.modeltype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThBNDvT9RO-P"
      },
      "source": [
        " By default the network in the NN_predictor is not a full tensorflow object but rather an array of the weights and biases of the different layers. It can still be used to make predictions while being less memory intensive by using matrix multiplication:\n",
        "\n",
        "        a = input\n",
        "        for layer in self.network:\n",
        "            a = a.clip(0)\n",
        "            a = ((a @ layer[0]) + layer[1])\n",
        "        prediction =  1 / (1 + np.exp(-a))#sigmoid(a)\n",
        "        \n",
        "Which is build into the NN_Predictor class. This is not really neccesary to know, but its a neat way to get insight in neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lXw0LJ3_qdX"
      },
      "source": [
        "### Challenge:\n",
        "Train a neural network specialised for the Shawanella genus.\n",
        "\n",
        " (Note: I have provided a full training dataset of all Shawanella models as `./files/shewanella_training_dataset.csv`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Iasf2mWGZYR"
      },
      "outputs": [],
      "source": [
        "# @title Solution\n",
        "#Load in a our training sample\n",
        "shewanella_training_data_path = './files/shewanella_training_data.csv'\n",
        "shewanella_training_data = pd.read_csv(training_data_path, index_col=0)\n",
        "\n",
        "#set save path\n",
        "save_path = 'NN_shewanella.npz'\n",
        "\n",
        "#Train the network\n",
        "NN_shewanella = NN_Trainer.train(data=shewanella_training_data, modeltype='ModelSEED',output_path=save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogXUGoRIq6mK"
      },
      "source": [
        "## Making predictions\n",
        "\n",
        "As expected we can now use the network to make predictions directly using the predict funciton or have the gapfiller use the new network to gapfill models using the `trainedNNPath` argument from the gapfill class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdDyXp8pRO-S"
      },
      "outputs": [],
      "source": [
        "path_to_draft_model = './files/Bifidobacterium adolescentis_atcc_15703.sbml'\n",
        "draft_model = read_sbml_model(path_to_draft_model)\n",
        "p = NN_example.predict(draft_model)\n",
        "pd.Series(p).hist()\n",
        "gapfill_model_custom_nn = Gapfill(draftModel = path_to_draft_model, objectiveName = 'bio1', trainedNNPath='NN_phylum.npz')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs9g5wLDbdAW"
      },
      "source": [
        "### Challenge:\n",
        "\n",
        "Use your newly trained network to gapfill any of the Shawanella models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxBnG-dMbaN9"
      },
      "outputs": [],
      "source": [
        "# @title Solution\n",
        "path_to_draft_model = './files/models/shewanella_models/23_shewanella_colwelliana_gca_001735525.sbml'\n",
        "gapfill_model_shewanella = Gapfill(draftModel = path_to_draft_model, objectiveName = 'bio1', trainedNNPath='NN_shewanella.npz')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro8nhzIQRO-S"
      },
      "source": [
        "# For the interested: Changing feature generation parameters\n",
        "\n",
        "Basically you now know how to train networks but there are many additional changes you want to make during training.\n",
        "\n",
        "During training the `generate_feature` function will automatically generate the training dataset. You can change several parameters for the generation of the feature:\n",
        "\n",
        "1. You can change the number of times each training model is used (nuplo, default=30).\n",
        "2. You can change the range of deletion percentages (min_for and max_for, default = 0.05 and 0.3), which will be removed in equal sized steps based on the number of replicates.\n",
        "3. You can weigh the deletion of certain reactions (del_p, default=None).\n",
        "4. You can also add false reactions (min_con and max_con) during training and change the weights of addition (con_p)*\n",
        "\n",
        "*Note: we do not currently use this for dnngior as it would not work with the masking of input reactions as the mask does not differentiate between contamination and real reactions.\n",
        "\n",
        "In the following example we set nuplo to 300 instead of 30, and we vary deletion between 0.2 and 0.5.\n",
        "\n",
        "`custom_feature = NN_Trainer.generate_feature(training_data, nuplo=300, min_for = 0.2, max_for = 0.5)`\n",
        "\n",
        "For convenience you can give the parameters to the train function and they will be passed on.\n",
        "\n",
        "`network = NN_Trainer.train(data=training_data, nuplo=300, min_for=0.2, max_for=0.5, modeltype='ModelSEED',save=False)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuYPGYu_cZL4"
      },
      "source": [
        "### Challenge\n",
        "\n",
        "Train a neural network where the deletions are weighed by the frequency of the reaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XNMNp-34FpGx"
      },
      "outputs": [],
      "source": [
        "# @title Solution\n",
        "frequency = training_data.sum(axis=1)\n",
        "NN_Trainer.train(training_data, del_p = frequency, modeltype='ModelSEED', save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auy5YVuNdjpQ"
      },
      "source": [
        "# For the interested: Changing network parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKx7GkUuRO-U"
      },
      "source": [
        "Finally, you can rely on the default parameters to define the network which we optimised for our usecase, but for optimal perfomance on different datasets, you might want to change the hyperparameters (dropout, batch size), the architecture (nnodes, nlayers) or bias of predicted classes (bias0). You can also disable the masking of input positions during loss calculation (maskI=False).\n",
        "\n",
        "You can also provide a validation split which will set apart a part of your input data during training and calculate scores after to validate your network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KotDYkOaamzO"
      },
      "source": [
        "###Challenge:\n",
        "\n",
        "Train a network that can predict contamination instead of omissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "90iWNlPkRrfI"
      },
      "outputs": [],
      "source": [
        "# @title Solution\n",
        "network = NN_Trainer.train(data=training_data, maskI=False, min_con = 0.05, max_con=0.55, modeltype='ModelSEED',save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNTwOnxfRO-V"
      },
      "source": [
        "# For the extra interested: Tensorflow object\n",
        "\n",
        "By default the function returns a class with the simplified network but you very well might want instead the full Tensorflow network. To do this you can set return_full_network = True, which will change the NN_predictor to contain a Tensorflow network instead, there modeltype and rxn keys work the same. If you want to save this different class you need to change the file extension to .h5.\n",
        "\n",
        "If you set return_history = True it will also return the history of training for optimisation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0e0PPlaRO-V"
      },
      "outputs": [],
      "source": [
        "save_path = os.path.join('files', 'NN_phylum.h5')\n",
        "NN_tensorflow, history = NN_Trainer.train(data=training_data, return_full_network=True, modeltype='ModelSEED', output_path=save_path, return_history=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQS12HxyRO-V"
      },
      "source": [
        "The main reason we dont use the full network is to save space and time, we can see that the tensorflow object is four times the size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPRXTLiYRO-W"
      },
      "outputs": [],
      "source": [
        "!ls files/NN_phylum* -lh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7MbfnoyRO-W"
      },
      "outputs": [],
      "source": [
        "print(\"The summary of the network: \\n{}\".format(NN_tensorflow.network.summary()))\n",
        "print(\"The rxn_keys: \\n{}\".format(NN_tensorflow.rxn_keys))\n",
        "print(\"The Modeltype: {}\".format(NN_example.modeltype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX8JSySuRO-X"
      },
      "source": [
        "With the history you can do whatever you want, for example plot the loss function over the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YHIgjvVRO-Y"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "439rcoTzRO-T"
      },
      "source": [
        "By default the network will asume that your input (the data without deletions) should be what the network tries to predict. Alternatively, you can provide labels (the full set of reactions) for the network to try and predict. The following is a slightly convoluted way to have the network only predict missing reactions and have it predict reactions already present as 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVMSl-TOdg-s"
      },
      "outputs": [],
      "source": [
        "manual_feature = NN_Trainer.generate_feature(training_data.T, nuplo=30, min_con=0, max_con=0, min_for = 0.05, max_for = 0.3, del_p = None, con_p = None)\n",
        "manual_training_data = pd.DataFrame(columns = training_data.index, data=manual_feature)\n",
        "manual_labels = 1 - manual_feature\n",
        "\n",
        "network = NN_Trainer.train(data=manual_training_data, rxn_keys=training_data.index, labels=manual_labels, maskI=False, min_for=0.0, max_for=0.0, modeltype='ModelSEED',save=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}